{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import threading\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from multiprocessing import cpu_count\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from libpysal.weights import Kernel\n",
    "from collections import defaultdict\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combinations(step: float) -> list[list[Any]]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        step (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[List[Any]]: _description_\n",
    "    \"\"\"\n",
    "    combinations = []\n",
    "    for i in np.arange(0, 1 + step, step):\n",
    "        for j in np.arange(0, 1 - i + step, step):\n",
    "            k = 1.0 - i - j\n",
    "            i_rounded, j_rounded, k_rounded = round(i, 2), round(j, 2), round(k, 2)\n",
    "\n",
    "            # Check if k_rounded is within valid range and the summation equals 1.0\n",
    "            if 0 <= k_rounded <= 1 and i_rounded + j_rounded + k_rounded == 1.0:\n",
    "                combinations.append([i_rounded, j_rounded, k_rounded])\n",
    "\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_results(simulator_results):\n",
    "    x, y, z, values = [], [], [], []\n",
    "    for weight, avg_free in simulator_results.items():\n",
    "        x.append(weight[0])\n",
    "        y.append(weight[1])\n",
    "        z.append(weight[2])\n",
    "        values.append(avg_free)\n",
    "\n",
    "    return x, y, z, values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(simulator_results):\n",
    "    (\n",
    "        x,\n",
    "        y,\n",
    "        z,\n",
    "        values,\n",
    "    ) = order_results(simulator_results)\n",
    "    # Find the index of the maximum average free requests\n",
    "    max_index = np.argmax(values)\n",
    "\n",
    "    # Retrieve the corresponding weight combination\n",
    "    optimal_weights = (x[max_index], y[max_index], z[max_index])\n",
    "    max_free_requests = values[max_index]\n",
    "    return optimal_weights, max_free_requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinationCache:\n",
    "    def __init__(self, capacity: int, expiration_time=10, prepopulate=False):\n",
    "        self.cache = OrderedDict()\n",
    "        self.capacity = capacity\n",
    "        self.expiration_time = expiration_time\n",
    "        self.lock = threading.Lock()\n",
    "        if prepopulate:\n",
    "            self.prepopulate_cache()\n",
    "\n",
    "    def get(self, key: int) -> int:\n",
    "        if key not in self.cache:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.cache[key][0]\n",
    "\n",
    "    def put(self, keys: List[int]) -> None:\n",
    "        with self.lock:\n",
    "            # Update the counter of existing items\n",
    "            keys_to_delete = []\n",
    "            for k, (value, count) in self.cache.items():\n",
    "                if count <= 1:\n",
    "                    keys_to_delete.append(k)\n",
    "                else:\n",
    "                    self.cache[k] = (value, count - 1)\n",
    "\n",
    "            # Remove items whose counter has reached zero\n",
    "            for k in keys_to_delete:\n",
    "                del self.cache[k]\n",
    "\n",
    "            for key in keys:\n",
    "                # Check if the cache is already full\n",
    "                if len(self.cache) >= self.capacity:\n",
    "                    # Remove the least recently used item from the cache\n",
    "                    self.cache.popitem(last=False)\n",
    "\n",
    "                # Put the new items in the cache with a counter of expiration_time\n",
    "                self.cache[key] = (key, self.expiration_time)\n",
    "                self.cache.move_to_end(key)\n",
    "\n",
    "    def prepopulate_cache(self) -> None:\n",
    "        keys = random.sample(range(886), self.capacity)\n",
    "        for key in keys:\n",
    "            self.cache[key] = (key, self.expiration_time)\n",
    "\n",
    "    def current_state(self) -> list[Any]:\n",
    "        return list(self.cache.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCache:\n",
    "    def __init__(self, expiration_time):\n",
    "        self.cache = OrderedDict()\n",
    "        self.expiration_time = expiration_time\n",
    "\n",
    "    def get(self, key: int) -> int:\n",
    "        if key not in self.cache:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.cache[key][0]\n",
    "\n",
    "    def put(self, keys: List[int]) -> None:\n",
    "        # Update the counter of existing items\n",
    "        keys_to_delete = []\n",
    "        for k, (value, count) in self.cache.items():\n",
    "            if count <= 1:\n",
    "                keys_to_delete.append(k)\n",
    "            else:\n",
    "                self.cache[k] = (value, count - 1)\n",
    "\n",
    "        # Remove items whose counter has reached zero\n",
    "        for k in keys_to_delete:\n",
    "            del self.cache[k]\n",
    "\n",
    "        # Put the new items in the cache with a counter of expiration_time\n",
    "        for key in keys:\n",
    "            self.cache[key] = (key, self.expiration_time)\n",
    "\n",
    "    def current_state(self) -> list[Any]:\n",
    "        return list(self.cache.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRUCache:\n",
    "    def __init__(self, capacity: int, prepopulate=False):\n",
    "        self.cache = OrderedDict()\n",
    "        self.capacity = capacity\n",
    "        if prepopulate:\n",
    "            self.prepopulate_cache()\n",
    "\n",
    "    def get(self, key: int) -> int:\n",
    "        if key not in self.cache:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.cache[key]\n",
    "\n",
    "    def put(self, keys: List[int]) -> None:\n",
    "        for key in keys:\n",
    "            # Check if the cache is already full\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                # Remove the least recently used item from the cache\n",
    "                self.cache.popitem(last=False)\n",
    "\n",
    "            # Add the new key or update the existing key, and move it to the end\n",
    "            self.cache[key] = key\n",
    "            self.cache.move_to_end(key)\n",
    "\n",
    "    def prepopulate_cache(self) -> None:\n",
    "        keys = random.sample(range(886), self.capacity)\n",
    "        for key in keys:\n",
    "            self.cache[key] = key\n",
    "\n",
    "    def current_state(self) -> list[Any]:\n",
    "        return list(self.cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloSimulation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights: list[float],\n",
    "        num: int,\n",
    "        cache_type,\n",
    "        param,\n",
    "        counties_gdf,\n",
    "        states_gdf,\n",
    "        divisions_gdf,\n",
    "        prepopulate_cache: bool = True,\n",
    "        return_type: str = \"requests\",\n",
    "    ) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            weights (list[Any]): _description_\n",
    "            num (int): _description_\n",
    "            hot_layer_constraint (_type_): _description_\n",
    "            preload_data (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.states_gdf = states_gdf\n",
    "        self.divisions_gdf = divisions_gdf\n",
    "        self.counties_gdf = counties_gdf\n",
    "\n",
    "        self.weights = weights\n",
    "        self.num = num\n",
    "        self.return_type = return_type\n",
    "        if cache_type == \"LRUCache\":\n",
    "            self.cache = LRUCache(param, prepopulate_cache)\n",
    "        elif cache_type == \"TimeCache\":\n",
    "            self.cache = TimeCache(param)\n",
    "        elif cache_type == \"CombinationCache\":\n",
    "            self.cache = CombinationCache(param, prepopulate=prepopulate_cache)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid cache type. Use 'LRUCache' or 'TimeCache'.\")\n",
    "        self.load_data()\n",
    "        self.load_stochastic_probabilities()\n",
    "\n",
    "    def load_dict_from_file(self, filename: str) -> Dict:\n",
    "        current_dir = Path.cwd()\n",
    "        data_dicts = (Path(current_dir) / \"dictionaries\").resolve()\n",
    "        \"\"\"Load dictionary from a file.\"\"\"\n",
    "        with Path.open(data_dicts / filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def load_csv_from_file(self, filename):\n",
    "        current_dir = Path.cwd()\n",
    "        stochastic_paths = (Path(current_dir) / \"stochastic_paths\").resolve()\n",
    "        with Path.open(stochastic_paths / filename, mode=\"rb\") as f:\n",
    "            return pd.read_csv(f)\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load data from the pickled dictionaries.\"\"\"\n",
    "        self.regions_data = self.load_dict_from_file(\"divisions_mapping.pkl\")\n",
    "        self.states_data = self.load_dict_from_file(\"states_mapping.pkl\")\n",
    "        self.counties_data = self.load_dict_from_file(\"counties_mapping.pkl\")\n",
    "\n",
    "    def load_stochastic_probabilities(self):\n",
    "        self.counties_stochastic = self.load_csv_from_file(\"counties_stochastic.csv\")\n",
    "        self.states_stochastic = self.load_csv_from_file(\"states_stochastic.csv\")\n",
    "        self.divisions_stochastic = self.load_csv_from_file(\"divisions_stochastic.csv\")\n",
    "\n",
    "    def fetch_data(self, data: dict, count: int) -> dict[Any, Any]:\n",
    "        \"\"\"Fetch a subset of items from a dictionary.\n",
    "\n",
    "        Args:\n",
    "            data (dict): Source dictionary.\n",
    "            count (int): Maximum number of items to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A subset of the dictionary.\n",
    "        \"\"\"\n",
    "        # Since dictionaries are unordered, we'll convert the dictionary to a list of items,\n",
    "        # take the first 'count' items, and then convert it back to a dictionary.\n",
    "        return dict(list(data.items())[:count])\n",
    "\n",
    "    def monte_carlo_simulation(self, num_runs: int) -> list:\n",
    "        \"\"\"Execute the Monte Carlo simulation for a specified number of runs using parallel threads.\n",
    "\n",
    "        Args:\n",
    "            num_runs (int): _description_\n",
    "\n",
    "        Returns:\n",
    "            float: _description_\n",
    "        \"\"\"\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "            # Use a loop to run the simulation num_runs times\n",
    "            futures = [executor.submit(self.run_sim) for _ in range(num_runs)]\n",
    "\n",
    "            results = []\n",
    "            for _i, f in enumerate(concurrent.futures.as_completed(futures), 1):\n",
    "                free_requests, _ = f.result()  # Extract only the free_requests_count\n",
    "                results.append(free_requests)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def kernelweights(self, gdf, k=15, steps=100, min_lag=3, max_lag=10):\n",
    "        kernel_weights = Kernel.from_dataframe(\n",
    "            gdf, k=k, fixed=False, function=\"gaussian\"\n",
    "        )\n",
    "        num_entries = len(gdf)\n",
    "        base_prob = 1e-6\n",
    "        prob_matrix = np.full((num_entries, steps), base_prob)\n",
    "        influence_decay = defaultdict(lambda: np.zeros(num_entries))\n",
    "\n",
    "        for t in range(steps):\n",
    "            # Apply decay from previous steps to the current base probability matrix\n",
    "            current_weights = np.full(\n",
    "                num_entries, base_prob\n",
    "            )  # Start with base probability each step\n",
    "\n",
    "            if t in influence_decay:\n",
    "                # Add the decay amounts to the base probabilities\n",
    "                current_weights += influence_decay[t]\n",
    "\n",
    "            selected_index = np.random.randint(num_entries)\n",
    "            weights = kernel_weights.sparse[selected_index, :].toarray().flatten()\n",
    "            weights += base_prob  # Ensure no zero probabilities\n",
    "\n",
    "            # Calculate decay for future periods\n",
    "            lag_duration = random.randint(min_lag, max_lag)\n",
    "            decay_amount = (\n",
    "                weights - base_prob\n",
    "            ) / lag_duration  # Only the excess probability decays\n",
    "\n",
    "            for future_t in range(1, lag_duration + 1):\n",
    "                if t + future_t < steps:\n",
    "                    influence_decay[t + future_t] += decay_amount\n",
    "\n",
    "            # Update the current step's probabilities with the freshly calculated weights\n",
    "            prob_matrix[:, t] = (\n",
    "                current_weights + weights\n",
    "            )  # Add the weights to current weights\n",
    "            prob_matrix[:, t] /= prob_matrix[\n",
    "                :, t\n",
    "            ].sum()  # Normalize probabilities to sum to 1\n",
    "\n",
    "        return prob_matrix\n",
    "\n",
    "    def run_sim(self) -> Tuple[int, List[Any]]:\n",
    "        \"\"\"Execute the simulation.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, List[Any]]: Tuple containing total count of free requests and history.\n",
    "        \"\"\"\n",
    "        free_scenes = 0\n",
    "        total_scenes = 0\n",
    "        free_requests = 0\n",
    "        history: List[Any] = []\n",
    "        counties_stochastic = self.kernelweights(self.counties_gdf, k=45, steps=100)\n",
    "        states_stochastic = self.kernelweights(self.states_gdf, k=3, steps=100)\n",
    "        divisions_stochastic = self.kernelweights(self.divisions_gdf, k=2, steps=100)\n",
    "\n",
    "        # Fetch subset of data\n",
    "        regions_subset = self.fetch_data(self.regions_data, 9)\n",
    "        states_subset = self.fetch_data(self.states_data, 49)\n",
    "        counties_subset = self.fetch_data(self.counties_data, 4437)\n",
    "\n",
    "        for i in range(self.num):\n",
    "            scale = np.random.choice([\"regions\", \"states\", \"counties\"], p=self.weights)\n",
    "            if scale == \"regions\":\n",
    "                data = regions_subset\n",
    "                probabilities = self.divisions_stochastic.iloc[:, i].values\n",
    "            elif scale == \"states\":\n",
    "                data = states_subset\n",
    "                probabilities = self.states_stochastic.iloc[:, i].values\n",
    "            else:\n",
    "                data = counties_subset\n",
    "                probabilities = self.counties_stochastic.iloc[:, i].values\n",
    "            # Ensure the keys in the data dictionary are in the same order as the DataFrame columns\n",
    "            feature_ids = list(data.keys())\n",
    "            feature_probabilities = probabilities[\n",
    "                : len(feature_ids)\n",
    "            ]  # Adjust in case of size mismatch\n",
    "\n",
    "            # Normalize probabilities\n",
    "            feature_probabilities = feature_probabilities / feature_probabilities.sum()\n",
    "\n",
    "            # Use the normalized probabilities to pick a feature_id\n",
    "            feature_id = np.random.choice(feature_ids, p=feature_probabilities)\n",
    "            landsat_scenes = data[feature_id]\n",
    "            moved_to_hot = False\n",
    "\n",
    "            for scene in landsat_scenes:\n",
    "                total_scenes += 1\n",
    "                if self.cache.get(scene) != -1:  # is found\n",
    "                    free_scenes += 1\n",
    "                elif self.cache.get(scene) == -1:  # is not found\n",
    "                    moved_to_hot = True\n",
    "\n",
    "            if not moved_to_hot:\n",
    "                free_requests += 1\n",
    "            self.cache.put(landsat_scenes)\n",
    "            history.append(self.cache.current_state())\n",
    "\n",
    "        free_ratio = free_scenes / total_scenes if total_scenes > 0 else 0\n",
    "\n",
    "        if self.return_type == \"ratio\":  # type: ignore\n",
    "            return free_ratio, history  # type: ignore\n",
    "        elif self.return_type == \"requests\":  # type: ignore\n",
    "            return free_requests, history  # type: ignore\n",
    "        elif self.return_type == \"scenes\":  # type: ignore\n",
    "            return free_scenes, history  # type: ignore\n",
    "        else:\n",
    "            raise ValueError(\"Invalid return type specified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(\n",
    "    num_requests,\n",
    "    weights,\n",
    "    cache_type,\n",
    "    parameters_list,\n",
    "    num_runs,\n",
    "    prepopulate_cache,\n",
    "    return_type,\n",
    "):\n",
    "    # Define the base directory where your data is located\n",
    "    current_dir = Path.cwd()\n",
    "    print(current_dir)\n",
    "    data = (Path(current_dir) / \"data\").resolve()\n",
    "    # Paths for the shapefiles\n",
    "    divisions_path = os.path.join(data, \"USA_Divisions\", \"usa_divisions.shp\")\n",
    "    counties_path = os.path.join(data, \"USA_Counties\", \"usa_counties.shp\")\n",
    "    states_path = os.path.join(data, \"USA_States\", \"usa_states.shp\")\n",
    "\n",
    "    # Reading the shapefiles using geopandas\n",
    "    divisions = gpd.read_file(divisions_path)\n",
    "    counties = gpd.read_file(counties_path)\n",
    "    states = gpd.read_file(states_path)\n",
    "\n",
    "    counties_df = pd.DataFrame()\n",
    "    counties_df[\"FIPS\"] = counties[\"FIPS\"]\n",
    "    counties_df[\"geometry\"] = counties[\"geometry\"]\n",
    "\n",
    "    states_df = pd.DataFrame()\n",
    "    states_df[\"FIPS\"] = states[\"STATEFP\"]\n",
    "    states_df[\"geometry\"] = states[\"geometry\"]\n",
    "\n",
    "    divisions_df = pd.DataFrame()\n",
    "    divisions_df[\"FIPS\"] = divisions.index\n",
    "    divisions_df[\"geometry\"] = divisions[\"geometry\"]\n",
    "    states_gdf = gpd.GeoDataFrame(states_df, geometry=\"geometry\")\n",
    "    counties_gdf = gpd.GeoDataFrame(counties_df, geometry=\"geometry\")\n",
    "    divisions_gdf = gpd.GeoDataFrame(divisions_df, geometry=\"geometry\")\n",
    "    states_gdf.crs = \"EPSG:4326\"\n",
    "    divisions_gdf.crs = \"EPSG:4326\"\n",
    "    counties_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "    parameter_results = {}\n",
    "    for ijx, parameter in enumerate(parameters_list, start=1):\n",
    "        parameter = int(parameter)\n",
    "        wstart_time = time.time()\n",
    "        simulator = MonteCarloSimulation(\n",
    "            num=num_requests,\n",
    "            weights=weights,\n",
    "            cache_type=cache_type,\n",
    "            param=parameter,\n",
    "            counties_gdf = counties_gdf,\n",
    "            states_gdf = states_gdf,\n",
    "            divisions_gdf = divisions_gdf,\n",
    "            prepopulate_cache=prepopulate_cache,\n",
    "            return_type=return_type\n",
    "        )\n",
    "        results = simulator.monte_carlo_simulation(num_runs)\n",
    "        average_free_requests = sum(results) / num_runs\n",
    "        free_request_std = np.std(np.array(results))\n",
    "        free_request_sem = free_request_std / np.sqrt(num_runs)\n",
    "        parameter_results[parameter] = [average_free_requests, free_request_sem]\n",
    "\n",
    "        print(\n",
    "            f\"      Parameter Analysis {parameter} completed in {(time.time() - wstart_time):.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        print(f\"      Mean requests {average_free_requests}\")\n",
    "        print(f\"      variance of requests {free_request_std}\")\n",
    "        print(\"------------------------------------------\\n\")\n",
    "\n",
    "    print(\"------------------------------------------\\n\")\n",
    "\n",
    "    return parameter_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(weights) -> dict:\n",
    "    \"\"\"\n",
    "    This function runs the analysis for the Monte Carlo Simulation.\n",
    "    \"\"\"\n",
    "    num_requests = 100\n",
    "    weights = weights\n",
    "    cache_type = \"LRUCache\"\n",
    "    parameters_list = np.linspace(1, 886, 20)\n",
    "    init_time = time.time()\n",
    "    num_runs = 100\n",
    "    prepopulate_cache = True\n",
    "    return_type = \"requests\"\n",
    "\n",
    "    simulator_results = run_simulation(\n",
    "        num_requests=num_requests,\n",
    "        weights=weights,\n",
    "        cache_type=cache_type,\n",
    "        parameters_list=parameters_list,\n",
    "        num_runs=num_runs,\n",
    "        prepopulate_cache=prepopulate_cache,\n",
    "        return_type=return_type,\n",
    "    )\n",
    "    print(f\"Analysis completed in {(time.time() - init_time):.2f} seconds\")\n",
    "\n",
    "    return simulator_results  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_sim(weights):\n",
    "    results = run_analysis(weights)\n",
    "    x = np.array(list(results.keys())) / 8.6\n",
    "    values = list(results.values())\n",
    "    y = np.array([val[0] for val in values])\n",
    "    yerr = np.array([val[1] for val in values])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(\n",
    "        x,\n",
    "        y,\n",
    "        yerr=yerr,\n",
    "        fmt=\"o\",\n",
    "        color=\"b\",\n",
    "        ecolor=\"lightgray\",\n",
    "        elinewidth=3,\n",
    "        capsize=5,\n",
    "        capthick=2,\n",
    "        markersize=4,\n",
    "    )\n",
    "    # Calculate coefficients of the line of best fit\n",
    "    coefficients = np.polyfit(x, y, 2)  # 1 means linear (degree of the polynomial)\n",
    "    # Create a polynomial from coefficients, representing the line of best fit\n",
    "    polynomial = np.poly1d(coefficients)\n",
    "\n",
    "    # Generate y-values for the line of best fit based on x-values\n",
    "    y_fit = polynomial(x)\n",
    "    # Plot the line of best fit\n",
    "    plt.plot(x, y_fit, \"-\", color=\"red\")  # '-' specifies a solid line\n",
    "    y_mean = np.mean(y)\n",
    "    ss_tot = np.sum((y - y_mean) ** 2)  # Total sum of squares\n",
    "    ss_res = np.sum((y - y_fit) ** 2)  # Residual sum of squares\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    # Add R^2 as text on the plot\n",
    "    plt.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        f\"$R^2 = {r_squared:.3f}$\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=12,\n",
    "    )\n",
    "    plt.xticks(range(0, 100, 5))\n",
    "    plt.yticks(range(0, 100, 5))\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.title(\"Even Weight with Order 2 Line of Best Fit\")\n",
    "    plt.ylabel(\"Percent of Free Requests\")\n",
    "    plt.xlabel(\"Percent of Hot Layer in Relation to Cold Layer\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jackr\\dev\\Hot-Cold-Simulation\n"
     ]
    }
   ],
   "source": [
    "weights = [0.33, 0.33, 0.34]\n",
    "plot_single_sim(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
